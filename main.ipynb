{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662ceb3a-2296-4a77-a305-d4eb1c2611fa",
   "metadata": {},
   "source": [
    "# Integrated Future Trajectory Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c94c18-6a23-441f-a503-603c5128f75a",
   "metadata": {},
   "source": [
    "This notebook contains functions for:\n",
    "1. recording and updating all past trajectories (in a pandas data frame) when consecutively predicting future trajectories\n",
    "2. preparing inputs for M2I, including noise suppression and self-driving car selection\n",
    "3. calling M2I\n",
    "4. post-processing M2I outputs\n",
    "5. preparing inputs for MPC, including U-turn detection, distance ranking, and trajectory filtering\n",
    "6. calling MPC\n",
    "7. post-processing MPC outputs\n",
    "8. updating data frame using processed MPC outputs\n",
    "9. preparing inputs for common road visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4a874-98d3-4dd8-a54f-841f34fcedd9",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefd63d-d6a5-4da8-89ff-591eb8f23571",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Usage:\n",
    "**Change variabled in this section and run all remaining cells.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac9891-f696-4e69-8247-ddf691f0e145",
   "metadata": {},
   "source": [
    "### Start Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1722e998-57ad-40ac-8ef1-d2df809f35c3",
   "metadata": {},
   "source": [
    "Start Jupyter Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abed4d3-8436-4f10-87dd-1ced7b76d4cc",
   "metadata": {},
   "source": [
    "### 0. Module Switches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f017c-cdeb-4ed2-a189-f6fdaf743402",
   "metadata": {},
   "source": [
    "**USE_M2I** decides whether M2I will be used to generate future trajectories. When it is set to True, future trajectories will be generated by M2I. Otherwise, future trajectory will be obtained from ground truth. \n",
    "\n",
    "**USE_MPC** decides whether MPC will be used to post-process future trajectories. If it is set to True, future trajectories will be processed by MPC first before being stored as past trajector or being passed to common road for visualizations. Otherwise, M2I outputs will be directly used without being processed by MPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa39c32-2688-4a5d-8b2b-25e1ebc2ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9236521-6007-44ae-861e-a8093d3064fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_M2I = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5426c314-4e55-4cc3-ac12-02b195468099",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MPC = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2057076-8c03-43da-a028-cd378f8d497b",
   "metadata": {},
   "source": [
    "### 1. select a scene\n",
    "Change **MANUAL_SCENEE_NAME** to the name of the scene (the name of the folder without 'split' that contains .pickle files). **SUFFIX** is the name of the scene file, which is under **MANUAL_SCENEE_NAME**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c9925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c78f0d-71af-44db-ba05-f2b699a130b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls '/DATA1/liyang/M2I_2/data_split' | grep -v 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d96bef-aaec-45c9-81f4-eca1846fa793",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /DATA1/liyang/M2I_2/data_split_new/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ab7fe-0ae7-4586-a246-5ddaf0bec621",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /DATA1/liyang/M2I_2/data_split_new/14_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe8a01-f0cd-492a-9916-3ce2082e38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_SCENEE_NAME = '14_2'\n",
    "SUFFIX = 'yihzuang#14_2__all__right_line'\n",
    "\n",
    "\n",
    "print('Scene name is:', MANUAL_SCENEE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9248d5-07f0-4893-9fc4-69ee01a9a907",
   "metadata": {},
   "source": [
    "### 2. set the ID of the self\n",
    "Change **MANUAL_SDC_ID** to the id of the self-driving car. If set to -1, use the sdc id from input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe94bb-2d74-4644-b129-38523dfb17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_SDC_ID = -1\n",
    "\n",
    "\n",
    "print('Self-driving car ID is:', MANUAL_SDC_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb39a4-3747-4f88-9ae4-be2f87eecfae",
   "metadata": {},
   "source": [
    "### 3. set the velocity threshold for noise suppression\n",
    "Change **MANUAL_TH** to the threshold for noise suppression. The higher the stronger. Recommended range: [0.5, 3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539c893-1c47-431e-ad7f-b184b51f00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_TH = 2.0\n",
    "\n",
    "if MANUAL_TH != 0:\n",
    "    print('Threshold for noise suppression is:', MANUAL_TH)\n",
    "if (MANUAL_TH < 0.5 or MANUAL_TH > 3.0) and MANUAL_TH !=0:\n",
    "    print('Recommended range for noise suppression threshold is between 0.5 and 3.0')\n",
    "if MANUAL_TH == 0:\n",
    "    print('Disabled noise suppression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543ea01-2d0c-462a-8d93-c8fe5591ce31",
   "metadata": {},
   "source": [
    "### 4. set the velocity threshold for noise suppression before visualization\n",
    "A separate velocity threshold is used to suppress the noise for the entire trajectory before creating the final input for the visualization module. Here, noise suppression will only suppress the noise in directions and will not affect the x and y coordinates. Change **VIS_TH** here. Recommended: 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee806e-189c-4938-bb87-386d4acd50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_VIS_TH = 1.0\n",
    "\n",
    "print('Threshold for visualization yaw noise suppression is:', MANUAL_VIS_TH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f99551-d8c4-41f9-aeac-0e2f7509be7b",
   "metadata": {},
   "source": [
    "### 5. set the threshold for U-turn detection\n",
    "\n",
    "Set the threshold when detecting U-turns. **No need to change this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_MAX_DIFFERENCE = 90\n",
    "\n",
    "print('Threshold for U-turn detection is:', MANUAL_MAX_DIFFERENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979941f-ddf7-41db-83f4-839750198bf6",
   "metadata": {},
   "source": [
    "### 6. set prediction frequency\n",
    "\n",
    "M2I predicts 80 samples (8 seconds) each time. To updated the future trajectory more frequently, set **MANUAL_NUM_FUTURE_TO_DISCARD** to a smaller value. Recommended: 7.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609f031-3b58-4e33-8498-fc4432066cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_PREDICTION_FREQUENCY = 7.5\n",
    "\n",
    "\n",
    "MANUAL_NUM_FUTURE_TO_DISCARD = 80 - int(MANUAL_PREDICTION_FREQUENCY * 10)\n",
    "print('Update future trajectory every', (80 - MANUAL_NUM_FUTURE_TO_DISCARD) / 10, 'seconds')\n",
    "if MANUAL_PREDICTION_FREQUENCY <= 0 or MANUAL_PREDICTION_FREQUENCY >= 8:\n",
    "    print('Invalid frequency.')\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8fd4a-5eba-4660-a9dc-9404c7321267",
   "metadata": {},
   "source": [
    "### 7. debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f1d00-6ad9-4873-b4a5-a748d3f6dbd0",
   "metadata": {},
   "source": [
    "After debugging, **DEBUG** should be set to False, so that when using ground truth trajectories, the heading angles of cars are also from ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6afebf-7311-48c9-aa7a-66a0c1a326c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "\n",
    "\n",
    "EXPRESS_VIS = False\n",
    "if (DEBUG == False) and (USE_MPC == False and USE_M2I == False):\n",
    "    MANUAL_TH = 0\n",
    "    EXPRESS_VIS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26b797-b555-4220-a01a-243d0567b7f9",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fbb213-a08c-4dd8-9636-c732dc7c4403",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d018b1-6cf9-41da-9582-98c192e7aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger \n",
    "import pickle5 as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074a98b-083c-4051-b23a-8df18986986b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06449751-5fbf-4097-af49-ea7976a8dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_whole = '/DATA1/liyang/M2I_2/data_split_new/' + MANUAL_SCENEE_NAME + '/' + SUFFIX + '.pickle'\n",
    "path = '/DATA1/liyang/M2I_2/data_split_new/' + MANUAL_SCENEE_NAME + '_split/' + SUFFIX + '.pickle'\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    decoded_example_group = pickle.load(f)\n",
    "decoded_example_group = decoded_example_group\n",
    "\n",
    "with open(path_whole, 'rb') as f:\n",
    "    decoded_whole = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae5308-b79b-437a-9334-77b03ff6627d",
   "metadata": {},
   "source": [
    "### (temp) dummy traffic light key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb19317-cdaa-4136-9ccd-1eec2e9def45",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_traffic_light_keys = []\n",
    "for key in decoded_example_group.keys():\n",
    "    if key.find('traffic_light') > -1:\n",
    "        naive_traffic_light_keys.append(key)\n",
    "for naive_traffic_light_key in naive_traffic_light_keys:\n",
    "    slash_index = naive_traffic_light_key.find('/')\n",
    "    prefix = naive_traffic_light_key[:slash_index]\n",
    "    suffix = naive_traffic_light_key[slash_index+1:]\n",
    "    traffic_current_key = prefix + '/' + 'current' + '/' + suffix\n",
    "    traffic_future_key = prefix + '/' + 'future' + '/' + suffix\n",
    "    decoded_example_group[traffic_future_key] = decoded_example_group[naive_traffic_light_key]\n",
    "    decoded_example_group[traffic_current_key] = decoded_example_group[naive_traffic_light_key][:,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c782900-deba-426f-9b1a-90ad09c2706d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137de870-0a39-4cd5-9618-a45598d793dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def progressBar(i, max, text):\n",
    "    bar_size = 60\n",
    "    j = (i + 1) / max\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\n",
    "        f\"[{'=' * int(bar_size * j):{bar_size}s}] {int(100 * j)}%  {text}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def update_stdout(text_to_display):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(text_to_display)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def isNaN(num):\n",
    "    res = num != num\n",
    "    if type(res) != bool:\n",
    "        return res.all()\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def filter_dict_by_keys(old_dict: dict, desired_keys: list, start_index = None, end_index = None):\n",
    "    if start_index is None and end_index is None:\n",
    "        return {k:old_dict[k] for k in desired_keys}\n",
    "    else:\n",
    "        return {k:old_dict[k][start_index:end_index] for k in desired_keys}\n",
    "    \n",
    "def list_to_float(x):\n",
    "    if type(x) == list:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8053f4-6ab3-46fe-9086-c711f876e5d7",
   "metadata": {},
   "source": [
    "### velocity and yaw calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787bd9b4-b472-4d18-b84c-004873e61436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velocity & Direction Processing\n",
    "# Modified from Yuhang Zheng\n",
    "def velocity_yaw_future(old_decoded_example):\n",
    "    \n",
    "    \"\"\"\n",
    "    For future trajectories, calculate velocity, velocity yaw, \n",
    "    and bbox yaw using x and y values. The values of bbox yaw\n",
    "    are set to be the same as velocity yaw. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    old_decoded_example: dict\n",
    "        Input for M2I. \n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    decoded_example: dict\n",
    "        Input for M2I with updated velocity, velocity yaw, \n",
    "        and bbox_yaw for future trajectories. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoded_example = copy.deepcopy(old_decoded_example)\n",
    "    \n",
    "    x = decoded_example['state/future/x']\n",
    "    y = decoded_example['state/future/y']\n",
    "    velocity_x = np.zeros((x.shape[0], x.shape[1]))\n",
    "    velocity_y = np.zeros((x.shape[0], x.shape[1]))\n",
    "    vel_yaw = np.zeros((x.shape[0], x.shape[1]))\n",
    "    for i in range(x.shape[1]-1):\n",
    "        velocity_x[:,i] = 10*(x[:,i+1]-x[:,i])\n",
    "        velocity_y[:,i] = 10*(y[:,i+1]-y[:,i])\n",
    "    velocity_x[:,-1] = velocity_x[:,-2]\n",
    "    velocity_y[:,-1] = velocity_y[:,-2]\n",
    "    vel_yaw = np.arctan2(velocity_y, velocity_x)\n",
    "    \n",
    "    decoded_example['state/future/velocity_x'] = velocity_x\n",
    "    decoded_example['state/future/velocity_y'] = velocity_y\n",
    "    decoded_example['state/future/vel_yaw'] = vel_yaw\n",
    "    decoded_example['state/future/bbox_yaw'] = vel_yaw\n",
    "    \n",
    "    for key in decoded_example:\n",
    "        try:\n",
    "            if decoded_example[key].dtype == np.float64:\n",
    "                decoded_example[key] = np.float32(decoded_example[key])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return decoded_example\n",
    "\n",
    "def velocity_yaw_past(old_decoded_example):\n",
    "    \n",
    "    \"\"\"\n",
    "    For past trajectories, calculate velocity, velocity yaw, \n",
    "    and bbox yaw using x and y values. The values of bbox yaw\n",
    "    are set to be the same as velocity yaw. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    old_decoded_example: dict\n",
    "        Input for M2I. \n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    decoded_example: dict\n",
    "        Input for M2I with updated velocity, velocity yaw, \n",
    "        and bbox_yaw for past trajectories. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    decoded_example = copy.deepcopy(old_decoded_example)\n",
    "    \n",
    "    x = decoded_example['state/past/x']\n",
    "    y = decoded_example['state/past/y']\n",
    "    velocity_x = np.zeros((x.shape[0], x.shape[1]))\n",
    "    velocity_y = np.zeros((x.shape[0], x.shape[1]))\n",
    "    vel_yaw = np.zeros((x.shape[0], x.shape[1]))\n",
    "    for i in range(x.shape[1]-1):\n",
    "        velocity_x[:,i] = 10*(x[:,i+1]-x[:,i])\n",
    "        velocity_y[:,i] = 10*(y[:,i+1]-y[:,i])\n",
    "    \n",
    "    # # Handle near static cars\n",
    "    # for car in range(velocity_x.shape[0]):\n",
    "    #     for time in range(velocity_x.shape[1]):\n",
    "    #         if velocity_x\n",
    "    \n",
    "    \n",
    "    velocity_x[:,-1] = velocity_x[:,-2]\n",
    "    velocity_y[:,-1] = velocity_y[:,-2]\n",
    "    vel_yaw = np.arctan2(velocity_y, velocity_x)\n",
    "    \n",
    "    decoded_example['state/past/velocity_x'] = velocity_x\n",
    "    decoded_example['state/past/velocity_y'] = velocity_y\n",
    "    decoded_example['state/past/vel_yaw'] = vel_yaw\n",
    "    decoded_example['state/past/bbox_yaw'] = vel_yaw\n",
    "\n",
    "    for key in decoded_example:\n",
    "        try:\n",
    "            if decoded_example[key].dtype == np.float64:\n",
    "                decoded_example[key] = np.float32(decoded_example[key])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return decoded_example\n",
    "\n",
    "\n",
    "def velocity_yaw_current(old_decoded_example):\n",
    "    \n",
    "    \"\"\"\n",
    "    For current trajectories, set velocity, velocity yaw, \n",
    "    and bbox yaw. Their values are set to be the same as \n",
    "    the latest values in past trajectories. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    old_decoded_example: dict\n",
    "        Input for M2I. \n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    decoded_example: dict\n",
    "        Input for M2I with updated velocity, velocity yaw, \n",
    "        and bbox_yaw for current trajectories. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    decoded_example = copy.deepcopy(old_decoded_example)\n",
    "    \n",
    "    decoded_example['state/current/velocity_x'] = decoded_example['state/past/velocity_x'][:,-1:]\n",
    "    decoded_example['state/current/velocity_y'] = decoded_example['state/past/velocity_y'][:,-1:]\n",
    "    decoded_example['state/current/bbox_yaw'] = decoded_example['state/past/bbox_yaw'][:,-1:]\n",
    "    decoded_example['state/current/vel_yaw'] = decoded_example['state/past/vel_yaw'][:,-1:]\n",
    "    \n",
    "    return decoded_example\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a354e-0c78-43e0-a267-9a22470147d2",
   "metadata": {},
   "source": [
    "### noise suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accc667-1589-4835-8004-da8ad76403f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_suppression(dict_to_suppress, split, th=2.5):\n",
    "    \n",
    "    res = copy.deepcopy(dict_to_suppress)\n",
    "    \n",
    "    for car_index in range(len(res['state/id'])):\n",
    "        x = res['state/' + split + '/x'][car_index]\n",
    "        y = res['state/' + split + '/y'][car_index]\n",
    "        vel_x = res['state/' + split + '/velocity_x'][car_index]\n",
    "        vel_y = res['state/' + split + '/velocity_y'][car_index]\n",
    "        vel_yaw = np.arctan2(vel_y, vel_x)\n",
    "        \n",
    "        threashold = th\n",
    "        mask = (np.abs(vel_x) < threashold) * (np.abs(vel_y) < threashold)\n",
    "        for index in range(len(mask)):\n",
    "            current_x = x[index]\n",
    "            current_y = y[index]\n",
    "\n",
    "            if mask[index]:\n",
    "                found = False\n",
    "                for previous_index in range(index - 1, -1, -1):\n",
    "                    if not mask[previous_index]:\n",
    "                        x[index] = x[previous_index]\n",
    "                        y[index] = y[previous_index]\n",
    "                        vel_yaw[index] = np.arctan2(vel_y[previous_index], vel_x[previous_index])\n",
    "                        vel_x[index] = 0\n",
    "                        vel_y[index] = 0\n",
    "                        \n",
    "                        #mask[index] = False\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    # if nothing in the past was greater than threshold: use the first one\n",
    "                    x[index] = x[0]\n",
    "                    y[index] = y[0]\n",
    "                    vel_yaw[index] = vel_yaw[0]\n",
    "                    vel_x[index] = 0\n",
    "                    vel_y[index] = 0\n",
    "        \n",
    "        res['state/' + split + '/x'][car_index] = x\n",
    "        res['state/' + split + '/y'][car_index] = y\n",
    "        res['state/' + split + '/vel_yaw'][car_index] = vel_yaw\n",
    "        res['state/' + split + '/bbox_yaw'][car_index] = vel_yaw\n",
    "    \n",
    "    # modify current if split is past\n",
    "    if split == 'past':\n",
    "        res['state/current/velocity_x'] = res['state/past/velocity_x'][:,-1:]\n",
    "        res['state/current/velocity_y'] = res['state/past/velocity_y'][:,-1:]\n",
    "        res['state/current/bbox_yaw'] = res['state/past/bbox_yaw'][:,-1:]\n",
    "        res['state/current/vel_yaw'] = res['state/past/vel_yaw'][:,-1:]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917aa83-62de-4bf2-b581-2a3ecc96499d",
   "metadata": {},
   "source": [
    "### U-turn detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0d2ab-6221-44ee-8d33-c28b1fcd5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interval(input_angle, max_difference=90):\n",
    "    \n",
    "    interval_list = []\n",
    "    \n",
    "    low = input_angle - max_difference\n",
    "    high = input_angle + max_difference\n",
    "    \n",
    "    if low <= -180:\n",
    "        interval_list.append([-180, input_angle])\n",
    "        extra_low = low + 360\n",
    "        extra_high = 180\n",
    "        interval_list.append([extra_low, extra_high])\n",
    "    else:\n",
    "        interval_list.append([low, input_angle])\n",
    "        \n",
    "        \n",
    "    if high >= 180:\n",
    "        interval_list.append([input_angle, 180])\n",
    "        extra_low = -180\n",
    "        extra_high = high - 360\n",
    "        interval_list.append([extra_low, extra_high])\n",
    "    else:\n",
    "        interval_list.append([input_angle, high])\n",
    "        \n",
    "    return interval_list\n",
    "\n",
    "def yaw_to_degree(yaw):\n",
    "    degree = yaw * 180 / np.pi\n",
    "    return degree\n",
    "\n",
    "def detect_u_turn_in_one_trajectory(x, y, max_difference=90):\n",
    "    \n",
    "    assert len(x.shape) == 1 and len(y.shape) == 1\n",
    "    \n",
    "    exist_u_turn = False\n",
    "        \n",
    "    x = np.expand_dims(x,0)\n",
    "    y = np.expand_dims(y,0)   \n",
    "    velocity_x = np.zeros((x.shape[0], x.shape[1]))\n",
    "    velocity_y = np.zeros((x.shape[0], x.shape[1]))\n",
    "    vel_yaw = np.zeros((x.shape[0], x.shape[1]))\n",
    "    for i in range(x.shape[1]-1):\n",
    "        velocity_x[:,i] = 10*(x[:,i+1]-x[:,i])\n",
    "        velocity_y[:,i] = 10*(y[:,i+1]-y[:,i])\n",
    "    vel_yaw = np.arctan2(velocity_y, velocity_x)\n",
    "    vel_yaw = vel_yaw[0]\n",
    "    vel_yaw[-1] = vel_yaw[-2]\n",
    "        \n",
    "    yaw_in_degree = yaw_to_degree(vel_yaw)\n",
    "    for current_angle in yaw_in_degree:\n",
    "        valid_interval_list = calculate_interval(current_angle, max_difference=max_difference)\n",
    "        if current_angle == 0:\n",
    "            continue\n",
    "        for another_angle in yaw_in_degree:\n",
    "            if another_angle == 0:\n",
    "                continue\n",
    "            \n",
    "            in_at_least_one_interval = False\n",
    "            \n",
    "            for valid_interval in valid_interval_list:\n",
    "                if another_angle >= valid_interval[0] and another_angle <= valid_interval[1]:\n",
    "                    in_at_least_one_interval = True \n",
    "                    break\n",
    "            if not in_at_least_one_interval: \n",
    "                exist_u_turn = True\n",
    "                break\n",
    "                \n",
    "        if exist_u_turn:\n",
    "            break\n",
    "            \n",
    "    return exist_u_turn\n",
    "\n",
    "def find_u_turns(m2i_outputs, th=2.5, max_difference=45):\n",
    "    res = copy.deepcopy(m2i_outputs)\n",
    "    \n",
    "    # calculate velocity and yaw for the predicted future trajactories\n",
    "    res = velocity_yaw_future(res)\n",
    "    # noise suppression for predicted future trajectories\n",
    "    res = noise_suppression(res, 'future', th=th)\n",
    "    \n",
    "    # create an empty list to indicate which trajectories contain u-turns\n",
    "    contain_u_turns = []\n",
    "    \n",
    "    # detect u turns for each predicted future trajectory\n",
    "    for index in range(res['state/id'].shape[0]):\n",
    "        x = res['state/future/x'][index]\n",
    "        y = res['state/future/y'][index]\n",
    "        current_trajectory_has_u_turn = detect_u_turn_in_one_trajectory(x, y, max_difference=max_difference)\n",
    "        contain_u_turns.append(False)\n",
    "    \n",
    "    return np.array(contain_u_turns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1db40-c3ef-4580-acc9-b91785415dc5",
   "metadata": {},
   "source": [
    "### MPC data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868b974-39b9-4c59-96d2-f060e58905b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict_using_indices(input_dict, indices):\n",
    "    \n",
    "    # indices is an array of boolean values\n",
    "    \n",
    "    filtered_inputs = {}\n",
    "\n",
    "    for key in input_dict.keys():\n",
    "        if type(input_dict[key]) == str:\n",
    "            filtered_inputs[key] = input_dict[key]\n",
    "        elif 'traffic_light' in key:\n",
    "            filtered_inputs[key] = input_dict[key]\n",
    "        elif 'roadgraph_samples' in key:\n",
    "            filtered_inputs[key] = input_dict[key]\n",
    "        else:\n",
    "            filtered_inputs[key] = input_dict[key][indices]\n",
    "            \n",
    "    return filtered_inputs\n",
    "\n",
    "\n",
    "\n",
    "def update_dict_using_indices(original_dict, new_dict, indices):\n",
    "    \n",
    "    # indices is an array of boolean values\n",
    "    \n",
    "    updated_dict = {}\n",
    "    \n",
    "    for key in new_dict.keys():\n",
    "        if type(new_dict[key]) == str or 'traffic_light' in key or 'roadgraph_samples' in key:\n",
    "            updated_dict[key] = new_dict[key]\n",
    "        else:\n",
    "            if len(original_dict[key].shape) == 2 and 'future' in key:\n",
    "                \n",
    "                # Fix (MPC) outputs of incorrect shape\n",
    "                if len(new_dict[key].shape) == 1:\n",
    "                    new_dict[key] = np.float32(np.array([new_dict[key]]))\n",
    "                \n",
    "                # Number of samples for each trajectory in MPC outputs\n",
    "                num_samples = new_dict[key].shape[1]\n",
    "\n",
    "                # Update original dict (MPC inputs) using new dict (MPC outputs)\n",
    "                #######################################################\n",
    "                # Unable to assign value to numpy 2D array on the LHS\n",
    "                # This is an workaround\n",
    "                \n",
    "                # Copy left\n",
    "                left_copy = []\n",
    "                for row in range(original_dict[key].shape[0]):\n",
    "                    current_row = []\n",
    "                    for col in range(original_dict[key].shape[1]):\n",
    "                        current_row.append(original_dict[key][row, col])\n",
    "                    left_copy.append(current_row)\n",
    "\n",
    "                # update left\n",
    "                new_dict_row_index = 0\n",
    "                for row in range(len(left_copy)):\n",
    "                    need_to_update_current_row = indices[row]\n",
    "                    if need_to_update_current_row:\n",
    "                        for col in range(num_samples):\n",
    "                            left_copy[row][col] = new_dict[key][new_dict_row_index, col]\n",
    "                        new_dict_row_index += 1\n",
    "\n",
    "                # list of lists to a 2D array\n",
    "                inner_as_array = [np.array(row) for row in left_copy]\n",
    "                outer_as_array = np.array(inner_as_array)\n",
    "\n",
    "                # To float32\n",
    "                outer_as_array = np.float32(outer_as_array)\n",
    "\n",
    "                #######################################################\n",
    "                \n",
    "                updated_dict[key] = outer_as_array\n",
    "\n",
    "        \n",
    "            else: \n",
    "                # updated_dict[key][indices] = new_dict[key]\n",
    "                updated_dict[key] = original_dict[key]\n",
    "\n",
    "            \n",
    "    return updated_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rank_distance(filtered_mpc_inputs):\n",
    "    distance_list = []\n",
    "\n",
    "    for trajectory_index in range(filtered_mpc_inputs['state/id'].shape[0]):\n",
    "        all_x = filtered_mpc_inputs['state/future/x'][trajectory_index]\n",
    "        all_y = filtered_mpc_inputs['state/future/y'][trajectory_index]\n",
    "\n",
    "        trajectory_distance = 0.0\n",
    "\n",
    "        for point_index in range(1, all_x.shape[0]):\n",
    "\n",
    "            x = all_x[point_index]\n",
    "            y = all_y[point_index]\n",
    "\n",
    "            previous_x = all_x[point_index - 1]\n",
    "            previous_y = all_y[point_index - 1]\n",
    "\n",
    "            # Calculate distance\n",
    "            x_distance_square = np.square(x - previous_x)\n",
    "            y_distance_square = np.square(y - previous_y)\n",
    "\n",
    "            step_distance = np.sqrt(x_distance_square + y_distance_square)\n",
    "            trajectory_distance += step_distance\n",
    "\n",
    "        distance_list.append(trajectory_distance)\n",
    "\n",
    "    distance_list = np.array(distance_list)\n",
    "\n",
    "    order_descending = np.argsort(-distance_list)\n",
    "    \n",
    "    return order_descending\n",
    "\n",
    "def delete_dummy_from_dict(input_dict):\n",
    "    keys_to_delete = []\n",
    "    for key in input_dict.keys():\n",
    "        if 'dummy' in key:\n",
    "            keys_to_delete.append(key)\n",
    "    if len(keys_to_delete) > 0:\n",
    "        for key in keys_to_delete:\n",
    "            del input_dict[key]\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b0646-9fce-474d-b94c-45afc0ad3d6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b2bc2-0c53-4590-bbbd-12685fbff42d",
   "metadata": {},
   "source": [
    "### Set SDC ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71200bda-2f70-4b4d-981a-739af8ff9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MANUAL_SDC_ID == -1:\n",
    "    sdc_id = int(decoded_example_group['state/id'][np.where(decoded_example_group['state/is_sdc'])[0][0]])\n",
    "    MANUAL_SDC_ID = sdc_id\n",
    "else:\n",
    "    sdc_id = MANUAL_SDC_ID\n",
    "print('sdc_id is:', sdc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd69da-57e3-40e1-afaa-8157ac64a885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473f46db-10a8-4745-a7a4-34bafe7d8cd4",
   "metadata": {},
   "source": [
    "### Relative Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937b115-e2f7-4025-af05-f4951c92e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_INDEX_IN_WHOLE = 10\n",
    "START_INDEX_IN_WHOLE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17216944-b231-4804-abcd-e4ee9530b820",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ede711-7955-4fb1-9cdd-33e5ef6f8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_keys = ['x', 'y', 'length', 'width', 'bbox_yaw', 'vel_yaw', 'velocity_x', 'velocity_y']\n",
    "\n",
    "prefix_past = 'state/past'\n",
    "prefix_future = 'state/future'\n",
    "prefix_current = 'state/current'\n",
    "\n",
    "desired_keys_past = [prefix_past + '/' + key for key in naive_keys] \n",
    "desired_keys_future = [prefix_future + '/' + key for key in naive_keys] \n",
    "desired_keys_current = [prefix_current + '/' + key for key in naive_keys] \n",
    "\n",
    "all_column_names_for_df = desired_keys_past + desired_keys_current + desired_keys_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13409992-aa1f-4903-b2de-b691df8662f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_past = filter_dict_by_keys(decoded_example_group, ['state/id'] + desired_keys_past, start_index=0, end_index=4)\n",
    "dummy_future = filter_dict_by_keys(decoded_example_group, ['state/id'] + desired_keys_future, start_index=2, end_index=7)\n",
    "dummy_current = filter_dict_by_keys(decoded_example_group, ['state/id'] + desired_keys_current, start_index=0, end_index=4)\n",
    "dummy_current_2 = filter_dict_by_keys(decoded_example_group, ['state/id'] + desired_keys_current, start_index=2, end_index=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8fc25-9ef1-4421-a345-a21ee7a1e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "roadgraph_dict = {}\n",
    "for key in decoded_example_group.keys():\n",
    "    if key.find('roadgraph_samples') >= 0:\n",
    "        roadgraph_dict[key] = decoded_example_group[key]\n",
    "        \n",
    "traffic_light_dict = {}\n",
    "for key in decoded_example_group.keys():\n",
    "    if key.find('traffic_light') >= 0:\n",
    "        traffic_light_dict[key] = decoded_example_group[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdad58-73d3-47cd-829f-f7a92b648ebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Frame Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d677dd-0b66-4373-b22f-c69c82683a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_empty_df_using_dict(empty_df, past_dict_to_use=None, current_dict_to_use=None, future_dict_to_use=None):\n",
    "    if past_dict_to_use is not None:\n",
    "        for name in past_dict_to_use.keys():\n",
    "            if name == 'state/id':\n",
    "                continue\n",
    "            empty_df[name] = past_dict_to_use[name].tolist()\n",
    "    if current_dict_to_use is not None:\n",
    "        for name in current_dict_to_use.keys():\n",
    "            if name == 'state/id':\n",
    "                continue\n",
    "            empty_df[name] = current_dict_to_use[name]\n",
    "    if future_dict_to_use is not None:\n",
    "        for name in future_dict_to_use.keys():\n",
    "            if name == 'state/id':\n",
    "                continue\n",
    "            empty_df[name] = future_dict_to_use[name].tolist()\n",
    "    return empty_df\n",
    "\n",
    "def create_and_fill_df_for_ids(all_column_names_for_df, past_dict_to_use=None, current_dict_to_use=None, future_dict_to_use=None, ids=None):\n",
    "    \n",
    "    if ids is not None:\n",
    "        pass\n",
    "    elif past_dict_to_use is not None:\n",
    "        ids = past_dict_to_use['state/id']\n",
    "    elif current_dict_to_use is not None:\n",
    "        ids = current_dict_to_use['state/id']\n",
    "    elif future_dict_to_use is not None:\n",
    "        ids = future_dict_to_use['state/id']\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_df = pd.DataFrame({'state/id':ids})\n",
    "    for name in all_column_names_for_df:\n",
    "        new_df[name] = np.nan\n",
    "    new_df = fill_empty_df_using_dict(new_df, past_dict_to_use=past_dict_to_use, current_dict_to_use=current_dict_to_use, future_dict_to_use=future_dict_to_use)\n",
    "    \n",
    "    # Transform lists (with is expected to contain only one element) to float\n",
    "    for desired_key in desired_keys_current:\n",
    "        new_df[desired_key] = new_df[desired_key].apply(list_to_float)\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "def check_invalid_past(x):\n",
    "    return (pd.Series(x) == -1).all()\n",
    "    \n",
    "def check_valid_past(x):\n",
    "    return (pd.Series(x) != -1).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cedd272-74cd-4548-923d-a9c68dff8ab1",
   "metadata": {},
   "source": [
    "### Create and fill df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7c8cf-4677-41b5-a62f-6bb2231abc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "past = filter_dict_by_keys(decoded_example_group, ['state/id'] + desired_keys_past)\n",
    "current = filter_dict_by_keys(decoded_example_group, ['state/id'] + desired_keys_current)\n",
    "df = create_and_fill_df_for_ids(all_column_names_for_df, past_dict_to_use=past, current_dict_to_use=current, future_dict_to_use=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a127c-14fe-4259-bd64-6660dd296c76",
   "metadata": {},
   "source": [
    "### Drop invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0418a6-5ac1-4376-a43e-c2ea26b2dfc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop invalid rows\n",
    "df = df[df['state/past/x'].apply(check_valid_past)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59629071-ff4a-490f-92c7-d7a9615ceae8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions to Update Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ebada9-ccf3-46ab-be75-e1547afaa37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_of_id_in_df(df, object_id):\n",
    "    mask = df['state/id'] == object_id\n",
    "    id_already_in_df = mask.any()\n",
    "    if id_already_in_df:\n",
    "        index = df[mask].index[0]\n",
    "        return index\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# concate original and new data\n",
    "def update_cell(df, row_index_in_df, column_to_update, new_cell_data, replace_items=False):\n",
    "    \n",
    "    # update cell if id already in df\n",
    "    if row_index_in_df >= 0:\n",
    "        # Updating cells whose values are lists (past and future) by contatenating\n",
    "        if not replace_items:\n",
    "            if not isNaN(df[column_to_update].iloc[row_index_in_df]):\n",
    "                right_half = new_cell_data.tolist()\n",
    "                # when using current (float) to update past (list), change float to list before concatenating them\n",
    "                if not type(right_half) == list:\n",
    "                    right_half = [right_half]\n",
    "                df.at[row_index_in_df, column_to_update] = df[column_to_update].iloc[row_index_in_df] + right_half\n",
    "            else:\n",
    "                try:\n",
    "                    df.at[row_index_in_df, column_to_update] = new_cell_data.tolist()\n",
    "                except:\n",
    "                    df[column_to_update] = df[column_to_update].astype(object)\n",
    "                    df.at[row_index_in_df, column_to_update] = new_cell_data.tolist()\n",
    "        # Updating cells whose values are not floats (current) by overwriting\n",
    "        else:\n",
    "            if not isNaN(df[column_to_update].iloc[row_index_in_df]):\n",
    "                df.at[row_index_in_df, column_to_update] = new_cell_data\n",
    "            else:\n",
    "                df[column_to_update] = df[column_to_update].astype(object)\n",
    "                df.at[row_index_in_df, column_to_update] = new_cell_data\n",
    "\n",
    "    # does not work if id is not in df\n",
    "    else:\n",
    "        raise ValueError(\"row_index_in_df should be non-negative (id should already be in the df)\")\n",
    "        \n",
    "def update_row(df, row_index_in_df, columns_to_update, columns_of_new_data, new_data, index_in_new_data, replace_items=False):\n",
    "    \n",
    "    for i in range(len(columns_of_new_data)):\n",
    "        column = columns_of_new_data[i]\n",
    "        column_to_update = columns_to_update[i]\n",
    "        # no need to update id\n",
    "        if column == 'state/id':\n",
    "            continue\n",
    "        new_cell_data = new_data[column][index_in_new_data]        \n",
    "        update_cell(df, row_index_in_df, column_to_update, new_cell_data, replace_items=replace_items)\n",
    "        \n",
    "        \n",
    "\n",
    "def get_dict_for_new_ids_from_new_data(new_data, mask):\n",
    "    return {key:new_data[key][mask] for key in new_data.keys()}\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "new_data: TODO\n",
    "\"\"\"\n",
    "def update_df(df, new_data, map_from, map_to, replace_items=False):\n",
    "    # update df for ids that are already present\n",
    "    columns_of_new_data = list(new_data.keys())\n",
    "    columns_to_update = [item.replace(map_from, map_to) for item in columns_of_new_data]\n",
    "    new_ids = new_data['state/id']\n",
    "    indices_of_ids_that_are_not_in_df = []\n",
    "    for index_in_new_data in range(new_ids.shape[0]):\n",
    "        current_id = new_ids[index_in_new_data]\n",
    "        row_index_in_df = find_index_of_id_in_df(df, current_id)\n",
    "        # object id already in df \n",
    "        if row_index_in_df >= 0:\n",
    "            update_row(df, row_index_in_df, columns_to_update, columns_of_new_data, new_data, index_in_new_data, replace_items=replace_items)\n",
    "        else:\n",
    "            indices_of_ids_that_are_not_in_df.append(index_in_new_data)\n",
    "    \n",
    "    # create a df using for ids that are not already in df\n",
    "    mask = np.array([False] * new_data['state/id'].shape[0])\n",
    "    for index in indices_of_ids_that_are_not_in_df:\n",
    "        mask[index] = True\n",
    "    dict_for_new_ids_from_new_data = get_dict_for_new_ids_from_new_data(new_data, mask)\n",
    "    all_column_names_for_df = df.columns\n",
    "    new_df = pd.DataFrame({'state/id':dict_for_new_ids_from_new_data['state/id']}) \n",
    "    for name in all_column_names_for_df:\n",
    "        new_df[name] = np.nan\n",
    "    for name in dict_for_new_ids_from_new_data:\n",
    "        new_df[name.replace(map_from, map_to)] = dict_for_new_ids_from_new_data[name].tolist()    \n",
    "    \n",
    "    resulting_df = pd.concat([df, new_df])\n",
    "    resulting_df['state/id'] = resulting_df['state/id'].astype('int64')\n",
    "    \n",
    "    # Transform lists (with is expected to contain only one element) to float\n",
    "    for desired_key in desired_keys_current:\n",
    "        resulting_df[desired_key] = resulting_df[desired_key].apply(list_to_float)\n",
    "    \n",
    "    return resulting_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b619280-e550-4d82-9054-d3ffdaa77b15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions to Generate Inputs for Other Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554874df-60a5-4325-8fae-ce24744acb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices_in_whole(df, decoded_whole):\n",
    "    df_ids = df['state/id']\n",
    "    indices_in_whole = []\n",
    "    for df_id in df_ids:\n",
    "        indices_in_whole.append(np.where(decoded_whole['state/id'] == df_id)[0][0])\n",
    "    return indices_in_whole\n",
    "\n",
    "def find_heretofore_gt_trajectories(df, decoded_whole):\n",
    "    indices_in_whole = find_indices_in_whole(df, decoded_whole)\n",
    "    heretofore_trajectories = {}\n",
    "\n",
    "    gt_trajectory_keys = {'state/x', 'state/y'}\n",
    "\n",
    "    for key in gt_trajectory_keys:\n",
    "        heretofore_trajectories[key] = decoded_whole[key][indices_in_whole,:CURRENT_INDEX_IN_WHOLE]\n",
    "    return heretofore_trajectories\n",
    "\n",
    "def weight_prediction_by_gt(predicted_trajectories, gt_trajectories, gt_weight=0.5):\n",
    "    result_dict = copy.deepcopy(predicted_trajectories)\n",
    "    \n",
    "    assert gt_weight >= 0\n",
    "    assert gt_weight <= 1\n",
    "    prediction_weight = 1 - gt_weight\n",
    "    # preprocess prediction: setting corresponding values in predictions to -1 if gt is invalid\n",
    "    if gt_weight > 0:\n",
    "        predicted_trajectories['state/past/x'][gt_trajectories['state/x'] == -1] = -1\n",
    "        predicted_trajectories['state/past/y'][gt_trajectories['state/y'] == -1] = -1\n",
    "        \n",
    "    \n",
    "    result_dict['state/past/x'] = predicted_trajectories['state/past/x'] * prediction_weight + gt_trajectories['state/x'] * gt_weight\n",
    "    result_dict['state/past/y'] = predicted_trajectories['state/past/y'] * prediction_weight + gt_trajectories['state/y'] * gt_weight\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def get_sdc_future_trajectory(current_index, decoded_whole,sdc_id,num_future_samples=80):\n",
    "    sdc_id_index_in_whole = np.where(decoded_whole['state/id'] == sdc_id)[0][0]\n",
    "    \n",
    "    sdc_future_trajectory_dict = {}\n",
    "    \n",
    "    for key in desired_keys_future:\n",
    "        key_for_whole = key.replace('future/', '')\n",
    "        sdc_whole_trajectory = decoded_whole[key_for_whole][sdc_id_index_in_whole]\n",
    "        # no more future\n",
    "        if current_index+1 >= len(sdc_whole_trajectory):\n",
    "            sdc_future_trajectory = np.array([-1] * num_future_samples)\n",
    "        sdc_future_trajectory = sdc_whole_trajectory[current_index+1:current_index+1+num_future_samples]\n",
    "        # pad if no enough future\n",
    "        if len(sdc_future_trajectory) < num_future_samples:\n",
    "            paddings = np.array([-1] * (num_future_samples - len(sdc_future_trajectory)))\n",
    "            np.hstack([sdc_future_trajectory, paddings])\n",
    "        sdc_future_trajectory_dict[key] = sdc_future_trajectory\n",
    "        \n",
    "    return sdc_future_trajectory_dict\n",
    "\n",
    "\n",
    "def preserve_recent_past_samples(x, num_past_samples_to_preserve):\n",
    "    if isNaN(x):\n",
    "        return [-1] * num_past_samples_to_preserve\n",
    "    elif len(x) > num_past_samples_to_preserve:\n",
    "        return x[-num_past_samples_to_preserve:]\n",
    "    elif len(x) == num_past_samples_to_preserve:\n",
    "        return x\n",
    "    else:\n",
    "        nums_to_pad = num_past_samples_to_preserve - len(x)\n",
    "        return [-1] * nums_to_pad + x\n",
    "\n",
    "    \n",
    "def float_to_list(x):\n",
    "    if isNaN(x):\n",
    "        return [-1]\n",
    "    else: \n",
    "        return [x]\n",
    "    \n",
    "    \n",
    "def df_column_to_2d_array(df_column):\n",
    "    df_column = df_column.tolist()\n",
    "    df_column = [np.array(v) for v in df_column]\n",
    "    try:\n",
    "        for i in range(len(df_column)):\n",
    "            if len(df_column[i].shape) == 2 and df_column[i].shape[1] ==  1 and df_column[i].shape[0] == 1:\n",
    "                df_column[i] = df_column[i][0]\n",
    "            \n",
    "        two_d_array = np.stack(df_column)\n",
    "    except:\n",
    "        print([len(k) for k in df_column])\n",
    "        print(df_column)\n",
    "        two_d_array = np.stack(df_column)\n",
    "    if len(two_d_array.shape) > 2:\n",
    "        two_d_array = two_d_array[:,:,0]\n",
    "    return two_d_array \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7a6cc-4699-4cc7-90a0-dda15fd9859c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### M2I post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6bce8-cb72-4198-b0cf-d5660fbe4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2i_inputs_post_processing(res,decoded_example_group):\n",
    "    ids = res['state/id']\n",
    "    indices_in_decoded_example = []\n",
    "    for current_id in ids:\n",
    "        current_index_in_decoded_example = np.where(decoded_example_group['state/id'] == current_id)[0][0]\n",
    "        indices_in_decoded_example.append(current_index_in_decoded_example)\n",
    "    \n",
    "    \n",
    "    res['state/past/valid'] = np.float64(res['state/past/x'] != -1)\n",
    "    res['state/current/valid'] = np.float64(res['state/current/x'] != -1)\n",
    "    res['state/future/valid'] = np.float64(res['state/future/x'] != -1)\n",
    "    \n",
    "    res['state/past/z'] = res['state/past/x'] * 0\n",
    "    res['state/current/z'] = res['state/current/x'] * 0\n",
    "    res['state/future/z'] = res['state/future/x'] * 0\n",
    "    res['state/past/height'] = res['state/past/x'] * 0\n",
    "    res['state/current/height'] = res['state/current/x'] * 0\n",
    "    res['state/future/height'] = res['state/future/x'] * 0\n",
    "    res['state/past/vel_yaw'] = res['state/past/x'] * 0\n",
    "    res['state/current/vel_yaw'] = res['state/current/x'] * 0\n",
    "    res['state/future/vel_yaw'] = res['state/future/x'] * 0\n",
    "    res['state/past/timestamp_micros'] = res['state/past/x'] * 0\n",
    "    res['state/current/timestamp_micros'] = res['state/current/x'] * 0\n",
    "    res['state/future/timestamp_micros'] = res['state/future/x'] * 0\n",
    "    res['state/type'] = np.ones(res['state/id'].shape)\n",
    "    res['state/objects_of_interest'] = np.ones(res['state/id'].shape, dtype=np.int64)\n",
    "    res['state/tracks_to_predict'] = np.zeros(res['state/id'].shape)\n",
    "\n",
    "    for key in res:\n",
    "        try:\n",
    "            if res[key].dtype == np.float64:\n",
    "                res[key] = np.float32(res[key])\n",
    "        except:\n",
    "            pass\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55649a-3cad-4a31-9670-c78ebe23580a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### inputs for M2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de6dea-ea3e-435e-8f2d-7fb667bddebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_for_M2I(df, sdc_id, num_past_samples_to_preserve=10, num_future_samples=80):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract road information and part of trajectories from the \n",
    "    complete history to form inputs for M2I. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: a pandas dataframe \n",
    "        It stores the trajectory history, including the first 1.1s of \n",
    "        ground truth and trajectories predicted by the M2I module. \n",
    "        \n",
    "    sdc_id: int\n",
    "        The id of the self-driving car.\n",
    "        \n",
    "    num_past_samples_to_preserve: int\n",
    "        The number of past samples to include in the M2I input.\n",
    "        \n",
    "    num_future_samples: int\n",
    "        The number of future samples to include in the M2I input. \n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    copy_of_decoded_example_group: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as the road information, which can be used by the M2I\n",
    "        for future trajectory prediction. Items that are not available \n",
    "        in the trajectory history will be filled with invalid values \n",
    "        0's and -1's. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # need id, past, and current\n",
    "    all_desired_keys = ['state/id'] + desired_keys_past + desired_keys_current\n",
    "    desired_df = df[all_desired_keys].copy()\n",
    "    \n",
    "    # make sure the length of past is 10\n",
    "    for column in desired_keys_past:\n",
    "        desired_df[column] = desired_df[column].apply(lambda x: preserve_recent_past_samples(x, num_past_samples_to_preserve=num_past_samples_to_preserve))\n",
    "        \n",
    "    \n",
    "    past_states = {'state/id': np.array(desired_df['state/id'].tolist())}\n",
    "    current_states = {'state/id': np.array(desired_df['state/id'].tolist())}\n",
    "    combined_states = {'state/id': np.array(desired_df['state/id'].tolist())}\n",
    "        \n",
    "    # combine past with current\n",
    "    for naive_key in naive_keys:\n",
    "        past_key = prefix_past + '/' + naive_key\n",
    "        current_key = prefix_current + '/' + naive_key\n",
    "        past_item = desired_df[past_key]\n",
    "        current_item = desired_df[current_key].apply(float_to_list)\n",
    "        \n",
    "        # create array of shape [batch_size, num_past_samples_to_preserve]\n",
    "        past_item = df_column_to_2d_array(past_item)\n",
    "        \n",
    "        # create array of shape [batch_size, 1]\n",
    "        current_item = df_column_to_2d_array(current_item)\n",
    "        \n",
    "        past_states[past_key] = past_item\n",
    "        current_states[current_key] = current_item    \n",
    "        \n",
    "        combined_states[past_key] = past_item\n",
    "        combined_states[current_key] = current_item\n",
    "        \n",
    "    # generate a copy of decoded_example_group\n",
    "    copy_of_decoded_example_group = copy.deepcopy(decoded_example_group)\n",
    "    # updated states in the copy\n",
    "    for key in combined_states.keys():\n",
    "        copy_of_decoded_example_group[key] = combined_states[key]\n",
    "    # TODO: find index of sdc \n",
    "    temp = copy_of_decoded_example_group['state/id'] == sdc_id\n",
    "    index_of_sdc = np.where(temp)[0][0]\n",
    "    # update is_sdc\n",
    "    new_sdc = np.zeros(copy_of_decoded_example_group['state/id'].shape[0])\n",
    "    new_sdc[index_of_sdc] = 1\n",
    "    copy_of_decoded_example_group['state/is_sdc'] = new_sdc\n",
    "    # TODO: decide which locations in is_sdc are invalid and set them to -1's\n",
    "    # reshape future and fill with dummy data\n",
    "    for future_key in desired_keys_future:\n",
    "        past_key = future_key.replace('future', 'past')\n",
    "        past_shape = combined_states[past_key].shape\n",
    "        copy_of_decoded_example_group[future_key] = np.zeros([past_shape[0], num_future_samples]) - 1\n",
    "        \n",
    "    \n",
    "    # update the future trajectory for sdc\n",
    "    future_trajectory_for_sdc = get_sdc_future_trajectory(CURRENT_INDEX_IN_WHOLE,decoded_whole, sdc_id, num_future_samples=num_future_samples)\n",
    "    for key in future_trajectory_for_sdc:\n",
    "        new_value = np.zeros(copy_of_decoded_example_group[key].shape) - 1\n",
    "        new_value[index_of_sdc] = future_trajectory_for_sdc[key]\n",
    "        copy_of_decoded_example_group[key] = new_value\n",
    "    # fill missing inputs wiht invalid values\n",
    "    copy_of_decoded_example_group = m2i_inputs_post_processing(copy_of_decoded_example_group,decoded_example_group)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # calculate vel and yaw (past)\n",
    "    # copy_of_decoded_example_group = velocity_yaw_past(copy_of_decoded_example_group)\n",
    "    # # calculate vel and yaw (current)\n",
    "    # copy_of_decoded_example_group = velocity_yaw_current(copy_of_decoded_example_group)\n",
    "    # # calculate vel and yaw (future)\n",
    "    # copy_of_decoded_example_group = velocity_yaw_future(copy_of_decoded_example_group)\n",
    "    \n",
    "    \n",
    "    return copy_of_decoded_example_group\n",
    "\n",
    "\n",
    "def is_the_longest(x, max_length):\n",
    "    if len(x) == max_length:\n",
    "        return True\n",
    "    elif len(x) < max_length:\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError('Incorrect max length. ' + 'Max length is ' + str(len(x)) + ' instead of ' + str(max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9f51d-d6b3-493f-9c46-d5159a077555",
   "metadata": {},
   "source": [
    "### MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98149a94-9557-4efc-8ff8-95a5dea8a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_for_mpc(inputs_for_m2i, predicted_traj_x, predicted_traj_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare inputs for MPC.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs_for_m2i: dict\n",
    "        Inputs for M2I.\n",
    "        \n",
    "    \n",
    "     predicted_traj_x: numpy.ndarray\n",
    "         x coordinates of future trajectories predicted by M2I.\n",
    "         \n",
    "     predicted_traj_y: numpy.ndarray\n",
    "         y coordinates of future trajectories predicted by M2I.\n",
    "         \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sample_mpc_inputs: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as the road information, which can be used by the MPC.\n",
    "        Future trajectories are the same as those predicted by M2I.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    sample_mpc_inputs = copy.deepcopy(inputs_for_m2i)\n",
    "    \n",
    "    # update future trajectory using m2i outputs\n",
    "    sample_mpc_inputs['state/future/x'] = predicted_traj_x\n",
    "    sample_mpc_inputs['state/future/y'] = predicted_traj_y\n",
    "    \n",
    "    return sample_mpc_inputs\n",
    "\n",
    "def filter_inputs_for_mpc(sample_mpc_inputs, th=2.5, max_difference=90):\n",
    "    \n",
    "    \"\"\"\n",
    "    Filter out trajectories that cannot be processed by MPC, including U-turns\n",
    "    and trajectories with noisy samples. Noise suppression will be performed \n",
    "    first. Trajectories that are noisy and trajectories with U-turns after \n",
    "    noise suppression will be filtered out. Additionally, trajectories will\n",
    "    be ranked by trajectory distances in descending order. Their rankings \n",
    "    will be stored in dummy/roadgraph_samples/distance_descending_order. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_mpc_inputs: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as the road information, which can be used by the MPC.\n",
    "        \n",
    "    th: float\n",
    "        The strength of noise suppression. The larger the value is, the\n",
    "        stronger the noise suppression is. \n",
    "        \n",
    "    max_difference: float\n",
    "        Maximum allowed direction differences (in degree) for any pair of object\n",
    "        locations (by x and y) in each future trajectory. If any pair of object \n",
    "        location's direction difference is greater than this value, the trajectory\n",
    "        will be filtered out from MPC inputs. \n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered_mpc_inputs: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as the road information, which can be used by the MPC.\n",
    "        Trajectories with U-turns or noise will are removed. \n",
    "    \n",
    "    mpc_valid_indices: numpy.ndarray\n",
    "        An array of Trues and Falses indicating whether each trajectory \n",
    "        in the input sample_mpc_inputs has been filtered out by this function. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # find out valid indices for mpc\n",
    "    u_turn_indices = find_u_turns(sample_mpc_inputs, th=th, max_difference=max_difference)\n",
    "    mpc_valid_indices = np.invert(u_turn_indices)\n",
    "    \n",
    "    # filter out trajectories whose x's are all -1's \n",
    "    for i in range(sample_mpc_inputs['state/future/x'].shape[0]):\n",
    "        if (sample_mpc_inputs['state/future/x'][i] == -1).all():\n",
    "            mpc_valid_indices[i] = False\n",
    "    \n",
    "    # filter valid indices for mpc\n",
    "    filtered_mpc_inputs = filter_dict_using_indices(sample_mpc_inputs, mpc_valid_indices)\n",
    "    \n",
    "    # rank distance for mpc\n",
    "    order_descending = rank_distance(filtered_mpc_inputs)\n",
    "    filtered_mpc_inputs['dummy/roadgraph_samples/distance_descending_order'] = order_descending\n",
    "    \n",
    "    return filtered_mpc_inputs, mpc_valid_indices\n",
    "\n",
    "def process_mpc_outputs(sample_mpc_inputs, mpc_outputs, mpc_valid_indices):\n",
    "    \"\"\"\n",
    "    Remove key-value pairs whose key contain 'dummy.' Additionally, using MPC outputs to \n",
    "    update corresponding values in un-filtered MPC inputs. Specifically, some \n",
    "    trajectories were filtered out from un-filtered before giving to MPC. For those \n",
    "    that were not filtered out, MPC produces updated values. We use these updated \n",
    "    values to update corresponding values in un-filtered MPC inputs. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_mpc_inputs: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as the road information, which can be used by the MPC.\n",
    "        Future trajectories are the same as those predicted by M2I. It\n",
    "        contains all trajectories. No trajectory was filtered out. \n",
    "\n",
    "    mpc_outputs: dict\n",
    "        MPC outputs using filtered inputs.\n",
    "\n",
    "    mpc_valid_indices:\n",
    "        An array of Trues and Falses indicating whether each trajectory \n",
    "        in the input sample_mpc_inputs has was filtered out from filtered \n",
    "        MPC inputs. \n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict_to_updated_df: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as the road information. Future trajectories are either\n",
    "        the output of M2I (for filtered out trajectories that could not \n",
    "        be processed by MPC) or the output of MPC. This dict can be used\n",
    "        to update trajectory history. \n",
    "        \n",
    "    \"\"\"\n",
    "    mpc_outputs = delete_dummy_from_dict(mpc_outputs)\n",
    "    dict_to_updated_df = update_dict_using_indices(sample_mpc_inputs, mpc_outputs, mpc_valid_indices)\n",
    "    return dict_to_updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6c849-d64b-4d5e-b975-1ef62a71854e",
   "metadata": {},
   "source": [
    "### inputs for common road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91812758-fb38-44e0-9f43-54c391af1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_for_common_road(df,decoded_example_group, drop_shorter=True, remove_invalid=False):\n",
    "    # need past\n",
    "    all_desired_keys = ['state/id'] + desired_keys_past\n",
    "    desired_df = df[all_desired_keys].copy()\n",
    "    \n",
    "    all_past_states = {'state/id': np.array(desired_df['state/id'].tolist())}\n",
    "    \n",
    "    # Pad shorter ones\n",
    "    if not drop_shorter:\n",
    "        for column in desired_keys_past:\n",
    "            # pad past\n",
    "            max_length = np.array([len(x) for x in desired_df[column]]).max()\n",
    "            desired_df[column] = desired_df[column].apply(lambda x: preserve_recent_past_samples(x, num_past_samples_to_preserve=max_length))\n",
    "            # get column\n",
    "            past_item = desired_df[column]\n",
    "            # create an array of shape []\n",
    "            past_item = df_column_to_2d_array(past_item)\n",
    "            # add to dict\n",
    "            all_past_states[column] = past_item\n",
    "    # Drop shorter ones\n",
    "    else:\n",
    "        max_length = np.array([len(x) for x in desired_df['state/past/x']]).max()\n",
    "        desired_df = desired_df[desired_df['state/past/x'].apply(lambda x: is_the_longest(x, max_length=max_length))]\n",
    "        for column in desired_keys_past:\n",
    "            # get column\n",
    "            past_item = desired_df[column]\n",
    "            # create an array of shape []\n",
    "            past_item = df_column_to_2d_array(past_item)\n",
    "            # add to dict\n",
    "            all_past_states[column] = past_item\n",
    "            \n",
    "    # Add additional items to dict\n",
    "    # TODO\n",
    "    all_past_states['scenario/id'] = decoded_example_group['scenario/id']\n",
    "    all_past_states['state/is_sdc'] = all_past_states['state/id'] * 0\n",
    "    all_past_states['state/type'] = decoded_example_group['state/type']\n",
    "    \n",
    "    \n",
    "    # TODO: remove objects whose past or current contain invalid values (-1's)\n",
    "    if remove_invalid:\n",
    "        # remove invalid past\n",
    "        past_mask = all_past_states['state/past/x'][:,0] != -1\n",
    "        for key in all_past_states.keys():\n",
    "            if type(all_past_states[key]) != str:\n",
    "                all_past_states[key] = all_past_states[key][past_mask]\n",
    "        # remove invalid current\n",
    "        pass\n",
    "    \n",
    "    return all_past_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2766dc1-035c-4841-9c06-419f96eec5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(path):\n",
    "    if  not osp.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def save_for_commondroad(data,sdc_id,decoded_example_group,save_path,smoothed_save_path):\n",
    "    # inputs for common road\n",
    "    sample_common_road_inputs = get_inputs_for_common_road(data,decoded_example_group, drop_shorter=True)\n",
    "    # re-calculate past\n",
    "    sample_common_road_inputs = velocity_yaw_past(sample_common_road_inputs)\n",
    "    for k in sample_common_road_inputs:\n",
    "        if type(sample_common_road_inputs[k] == np.ndarray):\n",
    "            sample_common_road_inputs[k] = np.nan_to_num(sample_common_road_inputs[k])\n",
    "    \n",
    "    \n",
    "    # set sdc\n",
    "    sdc_index_in_common_road_inputs = np.where(sample_common_road_inputs['state/id'] == sdc_id)[0][0]\n",
    "    sample_common_road_inputs['state/is_sdc'][sdc_index_in_common_road_inputs] = 1\n",
    "    smoothed_common_road_inputs = noise_suppression(sample_common_road_inputs, 'past', th=MANUAL_TH)\n",
    "    # write to disk\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(sample_common_road_inputs, f)\n",
    "    with open(smoothed_save_path, 'wb') as f:\n",
    "        pickle.dump(smoothed_common_road_inputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d8527-f3a9-4331-a73b-9139f59b8ee3",
   "metadata": {},
   "source": [
    "### update df using mpc outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74583b0-4018-4584-a856-04de016e3254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_using_MPC_outputs(df, mpc_outputs, num_future_samples_to_use_as_future):\n",
    "    \n",
    "    \"\"\"\n",
    "    Update trajectory by adding newly predicted future trajectories to history. \n",
    "    Predicted trajectory and current locations of objects will be concatenated \n",
    "    to trajectories in trajectory history. The last sample in each predicted \n",
    "    trajectory will not be concatenated to trajectories in trajectory history. \n",
    "    Instead, they will be used to update the current location of each object. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.core.frame.DataFrame\n",
    "    \n",
    "        Trajectory history which contains object property, velocity, location, \n",
    "        and direction data. \n",
    "        \n",
    "    \n",
    "    mpc_outputs: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as road information. Future trajectories are either\n",
    "        the output of M2I (for filtered out trajectories that could not \n",
    "        be processed by MPC) or the output of MPC. \n",
    "    \n",
    "    \n",
    "    num_future_samples_to_use_as_future: int\n",
    "        The number of samples in future trajectories to discard. Discarded \n",
    "        samples will not be used to update trajectory history. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    updated_df: pandas.core.frame.DataFrame\n",
    "        Trajectory history which is updated using newly predicted future trajectories. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # need id, current, and future\n",
    "    #all_desired_keys = ['state/id'] + desired_keys_current + desired_keys_future\n",
    "    \n",
    "    new_past_states = {'state/id': mpc_outputs['state/id']}\n",
    "    new_future_states = {'state/id': mpc_outputs['state/id']}\n",
    "    new_current_states = {'state/id': mpc_outputs['state/id']}\n",
    "    \n",
    "    # get current and future:\n",
    "    for naive_key in naive_keys:\n",
    "        current_key = prefix_current + '/' + naive_key\n",
    "        future_key = prefix_future + '/' + naive_key\n",
    "        new_past_key = 'state/new_past' + '/' + naive_key\n",
    "        new_future_key = 'state/new_future' + '/' + naive_key\n",
    "        new_current_key = 'state/new_current' + '/' + naive_key\n",
    "        \n",
    "        if current_key not in mpc_outputs.keys() or future_key not in mpc_outputs.keys():\n",
    "            print('Missing ', current_key, future_key)\n",
    "            continue\n",
    "        \n",
    "        current_item = mpc_outputs[current_key]\n",
    "        future_item = mpc_outputs[future_key]\n",
    "        \n",
    "        new_past_item = np.hstack([current_item, future_item[:, :-num_future_samples_to_use_as_future-1]])\n",
    "        new_future_item = future_item[:, -num_future_samples_to_use_as_future:]\n",
    "        new_current_item = future_item[:, -num_future_samples_to_use_as_future-1:-num_future_samples_to_use_as_future]\n",
    "        \n",
    "        new_past_states[new_past_key] = new_past_item\n",
    "        new_future_states[new_future_key] = new_future_item\n",
    "        new_current_states[new_current_key] = new_current_item\n",
    "        \n",
    "    \n",
    "    # update df using generated dicts\n",
    "    # 1. concat past and new past\n",
    "    updated_df = update_df(df, new_past_states, map_from='new_past', map_to='past', replace_items=False)\n",
    "    # 2. overwrite future with new future\n",
    "    updated_df = update_df(updated_df, new_future_states, map_from='new_future', map_to='future', replace_items=True)\n",
    "    # 3. overwrite current with new current\n",
    "    updated_df = update_df(updated_df, new_current_states, map_from='new_current', map_to='current', replace_items=True)\n",
    "    # 4. update CURRENT_INDEX_IN_WHOL\n",
    "    time_passed = new_past_states['state/new_past/x'].shape[1]\n",
    "    global CURRENT_INDEX_IN_WHOLE\n",
    "    CURRENT_INDEX_IN_WHOLE += time_passed\n",
    "    \n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2159da5-54d4-42dc-a19f-59bb20329436",
   "metadata": {},
   "source": [
    "### extra un-sorted functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca8d32-fd9f-4277-8447-11e7b049e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_future_trajectory(future_length=80):\n",
    "    indices_in_whole = find_indices_in_whole(df, decoded_whole)\n",
    "    gt_future_x = decoded_whole['state/x'][indices_in_whole][:,CURRENT_INDEX_IN_WHOLE+1:CURRENT_INDEX_IN_WHOLE+1+future_length]\n",
    "    gt_future_y = decoded_whole['state/y'][indices_in_whole][:,CURRENT_INDEX_IN_WHOLE+1:CURRENT_INDEX_IN_WHOLE+1+future_length]\n",
    "    \n",
    "    return gt_future_x, gt_future_y\n",
    "\n",
    "\n",
    "def get_ground_truth_future_yaw(future_length=80):\n",
    "    gt_future_bbox_yaw = decoded_whole['state/bbox_yaw'][indices_in_whole][:,CURRENT_INDEX_IN_WHOLE+1:CURRENT_INDEX_IN_WHOLE+1+future_length]\n",
    "    gt_future_vel_yaw = decoded_whole['state/vel_yaw'][indices_in_whole][:,CURRENT_INDEX_IN_WHOLE+1:CURRENT_INDEX_IN_WHOLE+1+future_length]\n",
    "    \n",
    "    return gt_future_bbox_yaw, gt_future_vel_yaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0ed1d-66f9-4eb4-a8ce-8b6db0aa863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_m2i_inference_out(vv_out,vc_out,vp_out):\n",
    "\n",
    "     \n",
    "    if vc_out is not None :\n",
    "        vv_out = np.concatenate([vv_out,vc_out],axis=0)\n",
    "        \n",
    "    if vp_out is not None :\n",
    "        vv_out = np.concatenate([vv_out,vp_out],axis=0)\n",
    "\n",
    "    return vv_out\n",
    "\n",
    "def m2i_prediction(res,args,model,MANUAL_TH, trajectory_type=None):\n",
    "    predicted_traj_x, predicted_traj_y  = None ,None\n",
    "    if res['state/id'] is  not None:\n",
    "        # 1. update vel and yaw before running m2i\n",
    "        res = velocity_yaw_future(velocity_yaw_past(res))\n",
    "        # 2. remove oscilations in past and current (x and y based on vel x and vel y)\n",
    "        res = noise_suppression(res, 'past', th=MANUAL_TH)\n",
    "        # 3. run m2i \n",
    "        predicted_traj_x, predicted_traj_y = run_m2i_inference(args, model, res, trajectory_type) \n",
    "\n",
    "    return predicted_traj_x, predicted_traj_y \n",
    "\n",
    "\n",
    "def add_surplus_value(surplus_keys,src,target):\n",
    "\n",
    "    for key in surplus_keys:\n",
    "\n",
    "        if isinstance(src[key],str):\n",
    "            target[key] = src[key]\n",
    "        else:\n",
    "            target[key] = src[key].copy()\n",
    "            \n",
    "    return target\n",
    "\n",
    "def combine_list(src,key,src_shape):\n",
    "    tmp = None\n",
    "    for idx, x in enumerate(src):\n",
    "        if idx == 0:\n",
    "            tmp = np.array(x[key]).reshape([1,-1])\n",
    "        else :\n",
    "            tmp = np.concatenate([tmp,np.array(x[key]).reshape(1,-1)],axis=0)    \n",
    "    # if tmp is not None and tmp.shape[1]==1:\n",
    "    #     tmp = tmp.flatten()\n",
    "    if tmp is not None:\n",
    "        tmp = tmp.reshape(src_shape)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def combine_as_m2i_input(all_object_keys,parse_data,src):\n",
    "    tmp={}\n",
    "    for k in all_object_keys:\n",
    "        \n",
    "        tmp[k]=combine_list(parse_data,k,src[k].shape)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def other_keys(res):\n",
    "    all_object_keys = []\n",
    "    for k,v in res.items():\n",
    "        if not k.startswith('state'):\n",
    "            all_object_keys.append(k)\n",
    "    return all_object_keys\n",
    "\n",
    "\n",
    "def object_keys(res):\n",
    "    all_object_keys = []\n",
    "    for k,v in res.items():\n",
    "        if k.startswith('state'):\n",
    "            all_object_keys.append(k)\n",
    "    return all_object_keys\n",
    "\n",
    "def get_state_by_id(_id,src):\n",
    "    res = {}\n",
    "    for k,v in src.items():\n",
    "        if  k.startswith('state'):\n",
    "            res[k]=v[_id,...]\n",
    "    return res\n",
    "\n",
    "def split_data(res):\n",
    "    res_vehicle= []\n",
    "    res_cyclist= []\n",
    "    res_pedestrian= []\n",
    "    for idx , t in enumerate(res['state/type']):\n",
    "        if int(t) == 1 :\n",
    "            res_vehicle.append(get_state_by_id(int(idx),res))\n",
    "        elif int(t) == 2 :\n",
    "            res_pedestrian.append(get_state_by_id(int(idx),res))\n",
    "        elif int(t) == 3 :\n",
    "            res_cyclist.append(get_state_by_id(int(idx),res))\n",
    "\n",
    "\n",
    "    all_object_keys = object_keys(res)\n",
    "    surplus_keys = other_keys(res)\n",
    "    \n",
    "    res_vehicle = combine_as_m2i_input(all_object_keys,res_vehicle,res)\n",
    "    res_cyclist = combine_as_m2i_input(all_object_keys,res_cyclist,res)\n",
    "    res_pedestrian = combine_as_m2i_input(all_object_keys,res_pedestrian,res)\n",
    "\n",
    "    res_vehicle=add_surplus_value(surplus_keys,res,res_vehicle)\n",
    "    res_cyclist=add_surplus_value(surplus_keys,res,res_cyclist)\n",
    "    res_pedestrian=add_surplus_value(surplus_keys,res,res_pedestrian)\n",
    "    return res_vehicle,res_cyclist,res_pedestrian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f04003-da0b-4c09-9922-9e7fd5bcba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_future_in_dict(original_dict, desired_future_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reduce the number of samples in future trajectories for MPC.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    original_dict: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as the road information, which can be used by the MPC.\n",
    "        Future trajectories are the same as those predicted by M2I.\n",
    "      \n",
    "    desired_future_length: int\n",
    "        The desired number of samples in each future trajectory. If\n",
    "        its value is smaller than the number of samples in the \n",
    "        future trajectories in original_dict, extra samples at the \n",
    "        end of each future trajectory in original_dict will be removed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    shortened_dict: dict\n",
    "        A dict that contains the past, current, and future trajectory,\n",
    "        as well as the road information, which can be used by the MPC.\n",
    "        Future trajectories are shortened. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    shortened_dict = copy.deepcopy(original_dict)\n",
    "    \n",
    "    for key in shortened_dict.keys():\n",
    "        if 'future' in key and not 'traffic_light' in key:\n",
    "            shortened_dict[key] = shortened_dict[key][:,:desired_future_length]\n",
    "    \n",
    "    return shortened_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e8162-3628-4099-867a-4a3e26c72b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_gt_past_and_current_velocity_yaw_for_the_first_round(res):\n",
    "    index_mapping = find_indices_in_whole(df, decoded_whole)\n",
    "    res['state/past/vel_yaw'] = np.float32(decoded_example_group['state/past/vel_yaw'][index_mapping])\n",
    "    res['state/past/bbox_yaw'] = np.float32(decoded_example_group['state/past/bbox_yaw'][index_mapping])\n",
    "    res['state/past/velocity_x'] = np.float32(decoded_example_group['state/past/velocity_x'][index_mapping])\n",
    "    res['state/past/velocity_y'] = np.float32(decoded_example_group['state/past/velocity_y'][index_mapping])\n",
    "    \n",
    "    res['state/current/vel_yaw'] = np.float32(decoded_example_group['state/current/vel_yaw'][index_mapping])\n",
    "    res['state/current/bbox_yaw'] = np.float32(decoded_example_group['state/current/bbox_yaw'][index_mapping])\n",
    "    res['state/current/velocity_x'] = np.float32(decoded_example_group['state/current/velocity_x'][index_mapping])\n",
    "    res['state/current/velocity_y'] = np.float32(decoded_example_group['state/current/velocity_y'][index_mapping])\n",
    "    return res\n",
    "\n",
    "def add_gt_future_trajectory_as_dummy(res):\n",
    "    gt_traj_x, gt_traj_y = get_ground_truth_future_trajectory(future_length=80)\n",
    "    res['dummy/future/gt/x'] = gt_traj_x\n",
    "    res['dummy/future/gt/y'] = gt_traj_y\n",
    "    return res, gt_traj_x, gt_traj_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0609506-8e81-4ff9-b123-591caa37815a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trajectory Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0901ba-e67e-4d66-be8a-68bcd9a9c46b",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea21eb5-a745-4132-bf5b-3aae08b72caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M2I\n",
    "if USE_M2I:\n",
    "    from src.m2i_script import *\n",
    "    m2i_checkpoint_path = '/DATA2/lpf/baidu/additional_files_for_m2i/test0908_0/model_save/model.12.bin'\n",
    "    args, m2i_model = init_m2i(m2i_checkpoint_path, reactor_type = 'vehicle')\n",
    "# MPC\n",
    "if USE_MPC:\n",
    "    from src.m2i_mpc_ultra import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472dc69c-f9cf-4b0b-bd27-75cd9032868d",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fda8c5-9ca9-47fb-bc17-94973dfc7cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203d4bc-9c9c-4789-82db-05dd65e7e4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59b74f-df17-45a9-862d-92c7bedcf742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a25f6f-3fc5-4bad-99c3-5dd8243f226c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "281c19bd-10ad-4f03-9144-5272281ceacb",
   "metadata": {},
   "source": [
    "## Predict Future Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a76b44-b7ca-4b62-8756-421b82f3c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Debug Information\n",
    "\n",
    "mpc_output_vel_yaw_list = []\n",
    "mpc_output_bbox_yaw_list = []\n",
    "mpc_valid_index_list = []\n",
    "\n",
    "dicts_to_update_df = []\n",
    "list_for_mpc_valid_indices = []\n",
    "list_for_raw_mpc_outputs = []\n",
    "list_for_sample_mpc_inputs = []\n",
    "list_for_filtered_mpc_inputs = []\n",
    "\n",
    "list_for_res = []\n",
    "#######################################\n",
    "\n",
    "for i in range(999999):\n",
    "    \n",
    "    \n",
    "    prefix = 'Round ' + str(i) + ': '\n",
    "    update_stdout(prefix)\n",
    "    \n",
    "    # Get inputs for m2i\n",
    "    try:\n",
    "        res = get_inputs_for_M2I(df, sdc_id)\n",
    "    except:\n",
    "        print('Break when preparing outputs.')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # Prepare velocity and yaw\n",
    "    if i == 0: \n",
    "        # for the first round, use ground truth yaw\n",
    "        res = use_gt_past_and_current_velocity_yaw_for_the_first_round(res)\n",
    "        res = velocity_yaw_future(res)\n",
    "    else:\n",
    "        # otherwise, calculate vel and yaw and remove oscilations\n",
    "        # 1. update vel and yaw before running m2i\n",
    "        res = velocity_yaw_future(velocity_yaw_current(velocity_yaw_past(res)))\n",
    "        # 2. remove oscilations in past and current (x and y based on vel x and vel y)\n",
    "        res = noise_suppression(res, 'past', th=MANUAL_TH)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # find and add gt trajectory for all cars as dummy\n",
    "    res, gt_traj_x, gt_traj_y = add_gt_future_trajectory_as_dummy(res)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    # Debug\n",
    "    list_for_res.append(res)\n",
    "    ###############################################\n",
    "    \n",
    "    # [3. Run M2I]\n",
    "    if USE_M2I:\n",
    "        if (res['state/past/x'] == -1).all():\n",
    "            print('Break from M2I. M2I input contains no valid trajectory.')\n",
    "            break\n",
    "        trajectory_type = 'nearestGT' # (highestScore, nearestGT, nearestLane)       \n",
    "        try:\n",
    "            predicted_traj_x, predicted_traj_y = run_m2i_inference(args, m2i_model, res, trajectory_type) \n",
    "        except:\n",
    "            print('Break from M2I through except. TODO: check M2I inputs before calling M2I.')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        predicted_traj_x, predicted_traj_y = gt_traj_x, gt_traj_y\n",
    "        \n",
    "    # [4. Run MPC]\n",
    "    # 4.1 use M2I outputs to updated future trajectories\n",
    "    sample_mpc_inputs = get_inputs_for_mpc(res, predicted_traj_x, predicted_traj_y)\n",
    "    \n",
    "    \n",
    "    index_mapping = find_indices_in_whole(df, decoded_whole)\n",
    "\n",
    "    sample_mpc_inputs['state/dummy/complete_gt/x'] = decoded_whole['state/x'][index_mapping]\n",
    "    sample_mpc_inputs['state/dummy/complete_gt/y'] = decoded_whole['state/y'][index_mapping]\n",
    "    sample_mpc_inputs['state/dummy/complete_gt/velocity_x'] = decoded_whole['state/velocity_x'][index_mapping]\n",
    "    sample_mpc_inputs['state/dummy/complete_gt/velocity_y'] = decoded_whole['state/velocity_y'][index_mapping]\n",
    "    sample_mpc_inputs['state/dummy/complete_gt/bbox_yaw'] = decoded_whole['state/bbox_yaw'][index_mapping]\n",
    "    sample_mpc_inputs['state/dummy/complete_gt/vel_yaw'] = decoded_whole['state/vel_yaw'][index_mapping]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    # Debug\n",
    "    list_for_sample_mpc_inputs.append(sample_mpc_inputs)\n",
    "    ######################################################################\n",
    "    \n",
    "    if USE_MPC:\n",
    "        # 4.2 speed up MPC inference by removing the second half of each trajectory\n",
    "        shortened_mpc_inputs = shorten_future_in_dict(sample_mpc_inputs, desired_future_length=80-MANUAL_NUM_FUTURE_TO_DISCARD+1)\n",
    "        # 4.3 filter out trajectories that cannot be processed by MPC\n",
    "        filtered_mpc_inputs, mpc_valid_indices = filter_inputs_for_mpc(shortened_mpc_inputs, th=MANUAL_TH, max_difference=MANUAL_MAX_DIFFERENCE)\n",
    "        # 4.4 calculate future velocity for MPC\n",
    "        filtered_mpc_inputs = velocity_yaw_future(filtered_mpc_inputs)\n",
    "        # (Break if mpc inputs contains no trajectory)\n",
    "        if filtered_mpc_inputs['state/future/x'].shape[0] == 0:\n",
    "            print('Break from MPC. MPC input contains no trajectory.')\n",
    "            break\n",
    "            \n",
    "        ######################################################################    \n",
    "        list_for_filtered_mpc_inputs.append(filtered_mpc_inputs)\n",
    "        ######################################################################\n",
    "        \n",
    "        \n",
    "        # 4.5 call MPC\n",
    "        mpc_outputs = mpc_forward(filtered_mpc_inputs)\n",
    "        \n",
    "        ######################################################################\n",
    "        # Debug\n",
    "        mpc_output_vel_yaw_list.append(mpc_outputs['state/future/vel_yaw'])\n",
    "        mpc_output_bbox_yaw_list.append(mpc_outputs['state/future/bbox_yaw'])\n",
    "        mpc_valid_index_list.append(mpc_valid_indices)\n",
    "        \n",
    "        list_for_raw_mpc_outputs.append(mpc_outputs)\n",
    "        ######################################################################\n",
    "        # 4.6 Remove dummy and update MPC inputs (with all trajectories) using MPC outputs (with only filtered trajectories)\n",
    "        mpc_outputs =  process_mpc_outputs(sample_mpc_inputs, mpc_outputs, mpc_valid_indices)\n",
    "    else:\n",
    "        mpc_outputs = copy.deepcopy(sample_mpc_inputs)\n",
    "    \n",
    "    ######################################################################\n",
    "    # Debug\n",
    "    dicts_to_update_df.append(mpc_outputs)\n",
    "    if USE_MPC:\n",
    "        list_for_mpc_valid_indices.append(mpc_valid_indices)\n",
    "        \n",
    "    ######################################################################\n",
    "    \n",
    "    # [5. Update df]\n",
    "    df = update_df_using_MPC_outputs(df, mpc_outputs, num_future_samples_to_use_as_future=MANUAL_NUM_FUTURE_TO_DISCARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c340337-b543-423b-8dbc-3ce6335a4d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb676cbc-5c52-437c-9a42-e8dc448a7b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a08d41-4538-45e0-b3f1-9c373c5db7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677baf9-6468-4a51-ad1e-285bafdae72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875d19f-03ef-454e-99cf-b159d9a59015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72657a9d-70f0-4c99-9aaa-b4d8094013b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b42182-2d8d-4ace-ac51-d1799f96e7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299cc7c9-86dc-4984-83d4-405cef125727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0f2a9-9e2b-4f0d-97cb-072a05b0f749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c47c43-38d3-4b96-93c6-d6d96ca988d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5435069-a806-411c-b06a-ca6d362150c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211a4c9-9d4b-408a-b08b-5c2ad4ff5fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae574f7-4d72-4a38-a5a6-7a3a343699ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd24cca-7c6d-4aee-9cbc-d051453974f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657a7e8-977d-4038-8bc0-ba5f46bfc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls yang_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d513bd8-a317-4d9b-ae82-45ed3c82a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1830a-258b-4dac-b55f-f3b859aac1c9",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a9b86-bad7-49be-81f6-a07cef9a396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "BASE_SAVE_PATH = 'yang_logs/logs'\n",
    "SAVE_PATH = osp.join(BASE_SAVE_PATH,time.strftime(\"%Y:%m:%d\",time.gmtime(time.time()))+\"_\"+str(int(time.time()))) + '_' + MANUAL_SCENEE_NAME\n",
    "make_dirs(SAVE_PATH)\n",
    "\n",
    "\n",
    "save_path = osp.join(SAVE_PATH,'sample_common_road_inputs_%s_%s_%s_%s.pkl'%(MANUAL_SCENEE_NAME,MANUAL_SDC_ID, 'USE_MPC='+str(USE_MPC),'USE_M2I='+str(USE_M2I)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3de24f-a1d9-4435-abca-26e477ae7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs for common road\n",
    "sample_common_road_inputs = get_inputs_for_common_road(df,decoded_example_group, drop_shorter=True)\n",
    "# re-calculate past\n",
    "sample_common_road_inputs = velocity_yaw_past(sample_common_road_inputs)\n",
    "for k in sample_common_road_inputs:\n",
    "    if type(sample_common_road_inputs[k] == np.ndarray):\n",
    "        sample_common_road_inputs[k] = np.nan_to_num(sample_common_road_inputs[k])\n",
    "        \n",
    "if EXPRESS_VIS:\n",
    "    num_samples = sample_common_road_inputs['state/past/x'].shape[1]\n",
    "    indices = find_indices_in_whole(df, decoded_whole)\n",
    "    \n",
    "    sample_common_road_inputs['state/past/velocity_x'] = decoded_whole['state/velocity_x'][indices][:, :num_samples]\n",
    "    sample_common_road_inputs['state/past/velocity_y'] = decoded_whole['state/velocity_x'][indices][:, :num_samples]\n",
    "    sample_common_road_inputs['state/past/vel_yaw'] = decoded_whole['state/vel_yaw'][indices][:, :num_samples]\n",
    "    sample_common_road_inputs['state/past/bbox_yaw'] = decoded_whole['state/bbox_yaw'][indices][:, :num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3fb7a-52d6-4ea9-9f64-89a9fa3161ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0316f3c-3c8a-4f7b-aec2-8b20c420cd58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69995d2a-cb0d-4046-b459-14d933213cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d365a60-2d5e-445a-9d57-5970e602c134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb5348-0410-4320-9762-83269f5bec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vel and yaw (first 1.1s) before doing noise suppression\n",
    "\n",
    "if USE_MPC:\n",
    "    index_mapping = find_indices_in_whole(df, decoded_whole)\n",
    "    # update bbox_yaw\n",
    "    sample_common_road_inputs['state/past/bbox_yaw'][:,:11] = np.hstack([decoded_example_group['state/past/bbox_yaw'][index_mapping], decoded_example_group['state/current/bbox_yaw'][index_mapping]])\n",
    "    # update vel_yaw\n",
    "    sample_common_road_inputs['state/past/vel_yaw'][:,:11] = np.hstack([decoded_example_group['state/past/vel_yaw'][index_mapping], decoded_example_group['state/current/vel_yaw'][index_mapping]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220dfd87-dad1-4826-a0ef-cad7f489b2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58107f30-a568-4ba8-8e35-fdf2277772df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e39766-4dfe-4eb6-9ee2-43526ffdf67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7943eb-d308-4d82-ba4c-45af0f85ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sdc\n",
    "sdc_index_in_common_road_inputs = np.where(sample_common_road_inputs['state/id'] == sdc_id)[0][0]\n",
    "sample_common_road_inputs['state/is_sdc'][sdc_index_in_common_road_inputs] = 1\n",
    "smoothed_common_road_inputs = noise_suppression(sample_common_road_inputs, 'past', th=MANUAL_VIS_TH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8e473-fb14-4af8-8334-b26cf0c76a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd36f7d-f540-4ee3-b02d-b16ba294f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vel and yaw in MPC (after the first 1.1s)\n",
    "\n",
    "\n",
    "if USE_MPC:\n",
    "    debug_start_index = 11 # TODO: verify this\n",
    "    debug_stride = 80 - MANUAL_NUM_FUTURE_TO_DISCARD\n",
    "\n",
    "\n",
    "    key = 'state/future/bbox_yaw'\n",
    "    for i in range(len(list_for_raw_mpc_outputs)):\n",
    "        if len(list_for_raw_mpc_outputs[i][key].shape) == 1:\n",
    "            list_for_raw_mpc_outputs[i][key] = np.float32(np.array([list_for_raw_mpc_outputs[i][key]]))\n",
    "\n",
    "\n",
    "    list_for_new_arrays = []\n",
    "\n",
    "    for round_index in range(len(list_for_raw_mpc_outputs)):\n",
    "        start_index_for_current_round = debug_start_index + round_index * debug_stride\n",
    "\n",
    "\n",
    "        new_array = np.zeros([dicts_to_update_df[round_index][key].shape[0], list_for_raw_mpc_outputs[round_index][key].shape[1]])\n",
    "        new_array[list_for_mpc_valid_indices[round_index]] = list_for_raw_mpc_outputs[round_index][key]\n",
    "        smoothed_yaw = smoothed_common_road_inputs[key.replace('future', 'past')][np.invert(list_for_mpc_valid_indices[round_index]), start_index_for_current_round:start_index_for_current_round+debug_stride]\n",
    "        if round_index < len(list_for_raw_mpc_outputs) - 1:\n",
    "            new_array[np.invert(list_for_mpc_valid_indices[round_index])] = smoothed_yaw\n",
    "        else:\n",
    "            new_array[np.invert(list_for_mpc_valid_indices[round_index])][:,:-1] = smoothed_yaw\n",
    "        list_for_new_arrays.append(new_array)\n",
    "\n",
    "\n",
    "    result_bbox_yaw = np.hstack(list_for_new_arrays)[:,:-1]\n",
    "\n",
    "\n",
    "    ################\n",
    "\n",
    "\n",
    "    debug_start_index = 11 # TODO: verify this\n",
    "    debug_stride = 80 - MANUAL_NUM_FUTURE_TO_DISCARD\n",
    "\n",
    "\n",
    "    key = 'state/future/vel_yaw'\n",
    "    for i in range(len(list_for_raw_mpc_outputs)):\n",
    "        if len(list_for_raw_mpc_outputs[i][key].shape) == 1:\n",
    "            list_for_raw_mpc_outputs[i][key] = np.float32(np.array([list_for_raw_mpc_outputs[i][key]]))\n",
    "\n",
    "\n",
    "    list_for_new_arrays = []\n",
    "\n",
    "    for round_index in range(len(list_for_raw_mpc_outputs)):\n",
    "        start_index_for_current_round = debug_start_index + round_index * debug_stride\n",
    "\n",
    "\n",
    "        new_array = np.zeros([dicts_to_update_df[round_index][key].shape[0], list_for_raw_mpc_outputs[round_index][key].shape[1]])\n",
    "        new_array[list_for_mpc_valid_indices[round_index]] = list_for_raw_mpc_outputs[round_index][key]\n",
    "        smoothed_yaw = smoothed_common_road_inputs[key.replace('future', 'past')][np.invert(list_for_mpc_valid_indices[round_index]), start_index_for_current_round:start_index_for_current_round+debug_stride]\n",
    "        if round_index < len(list_for_raw_mpc_outputs) - 1:\n",
    "            new_array[np.invert(list_for_mpc_valid_indices[round_index])] = smoothed_yaw\n",
    "        else:\n",
    "            new_array[np.invert(list_for_mpc_valid_indices[round_index])][:,:-1] = smoothed_yaw\n",
    "        list_for_new_arrays.append(new_array)\n",
    "\n",
    "\n",
    "    result_vel_yaw = np.hstack(list_for_new_arrays)[:,:-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if USE_MPC:\n",
    "    sample_common_road_inputs['state/past/bbox_yaw'][:,11:] = result_bbox_yaw[:,:]\n",
    "    sample_common_road_inputs['state/past/vel_yaw'][:,11:] = result_vel_yaw[:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793ed4a-e142-4fce-9e9b-dfd8a6a0bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to disk\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(sample_common_road_inputs, f)\n",
    "    \n",
    "\n",
    "logger.info(f'Common Road inputs are saved at:\\n {save_path}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451b197-5431-4daa-b1d9-4e231f52f3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90a721-0077-4495-8841-892f8f9a8f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e400a54e-e19b-4ad7-9c6b-be999d0511ab",
   "metadata": {},
   "source": [
    "## Common Road Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6ef5f-e3c0-412e-9f37-8639add1343c",
   "metadata": {},
   "source": [
    "1. ssh into 10.0.0.14\n",
    "2. conda activate ~/../DISCOVER_summer2022/yanzj/.conda/envs/commonroad/\n",
    "3. put the named 'sample_common_road_inputs.pkl' from root@106.12.155.70 to the folder named \"inputs_for_commonroad\" in 10.0.0.14\n",
    "4. cd commonroad_fcn\n",
    "5. python output_trajectory.py\n",
    "6. use the path printed in the terminal to update path in frame2video_multi_thread.py\n",
    "7. run frame2video_multi_thread.py\n",
    "8. video stored in commonroad_fcn/frame2video_path/output_trajectory/CNN_PEK_data_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab30d0-79b8-4596-a108-24bfb943695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = save_path[:save_path.rfind('/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b0cb6-a087-4473-8893-5b75e09e2e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2ab9d-649a-4aa3-ac86-4a2e7fa21aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trajectory_visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276c1bf-d280-4db4-8e49-861c35596d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('video/output_trajectory', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b393c-9bca-4378-91ac-edbcb62af342",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "traj_visualization(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4a582-bdff-43c5-8a84-ef339b63327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Video saved under:\\n' + 'video/output_trajectory/data_video/' + result_dir[result_dir.rfind('/')+1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "443b2142a733eede8fd736f3b9262b5f15b6e24b554c8625ccf50f9ba8be612e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
